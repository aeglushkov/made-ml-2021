{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you submit this notebook, make sure everything runs as expected in the local test cases. \n",
    "Please, paste the solution to the designed cell and do not change anything else.\n",
    "\n",
    "Also, please, leave your first and last names below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FirstName = \"Александр\"\n",
    "LastName = \"Глушков\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40b231b30fc9f58984904a569710f504",
     "grade": false,
     "grade_id": "cell-ac8fc52d8a39ccb4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def entropy(y):  \n",
    "    \"\"\"\n",
    "    Computes entropy of the provided distribution. Use log(value + eps) for numerical stability\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array of type float with shape (n_objects, n_classes)\n",
    "        One-hot representation of class labels for corresponding subset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Entropy of the provided subset\n",
    "    \"\"\"\n",
    "    EPS = 0.0005\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    y = np.sum(y, axis=0) / y.shape[0]\n",
    "    entropy = -1 * np.sum(y * np.log(y + EPS))\n",
    "    \n",
    "    return entropy\n",
    "    \n",
    "def gini(y):\n",
    "    \"\"\"\n",
    "    Computes the Gini impurity of the provided distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array of type float with shape (n_objects, n_classes)\n",
    "        One-hot representation of class labels for corresponding subset\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Gini impurity of the provided subset\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    y = np.sum(y, axis=0) / y.shape[0]\n",
    "    gini_impurity = 1 - np.sum(y ** 2)\n",
    "    return gini_impurity\n",
    "    \n",
    "def variance(y):\n",
    "    \"\"\"\n",
    "    Computes the variance the provided target values subset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array of type float with shape (n_objects, 1)\n",
    "        Target values vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Variance of the provided target vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return np.var(y)\n",
    "\n",
    "def mad_median(y):\n",
    "    \"\"\"\n",
    "    Computes the mean absolute deviation from the median in the\n",
    "    provided target values subset\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : np.array of type float with shape (n_objects, 1)\n",
    "        Target values vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Mean absolute deviation from the median in the provided vector\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return np.mean(np.abs(y - np.median(y)))\n",
    "\n",
    "\n",
    "def one_hot_encode(n_classes, y):\n",
    "    y_one_hot = np.zeros((len(y), n_classes), dtype=float)\n",
    "    y_one_hot[np.arange(len(y)), y.astype(int)[:, 0]] = 1.\n",
    "    return y_one_hot\n",
    "\n",
    "\n",
    "def one_hot_decode(y_one_hot):\n",
    "    return y_one_hot.argmax(axis=1)[:, None]\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    This class is provided \"as is\" and it is not mandatory to it use in your code.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_index, threshold, proba=0):\n",
    "        self.feature_index = feature_index\n",
    "        self.value = threshold\n",
    "        self.proba = proba\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        \n",
    "        \n",
    "class DecisionTree(BaseEstimator):\n",
    "    all_criterions = {\n",
    "        'gini': (gini, True), # (criterion, classification flag)\n",
    "        'entropy': (entropy, True),\n",
    "        'variance': (variance, False),\n",
    "        'mad_median': (mad_median, False)\n",
    "    }\n",
    "\n",
    "    def __init__(self, n_classes=None, max_depth=np.inf, min_samples_split=2, \n",
    "                 criterion_name='gini', debug=False):\n",
    "\n",
    "        assert criterion_name in self.all_criterions.keys(), 'Criterion name must be on of the following: {}'.format(self.all_criterions.keys())\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        if criterion_name == \"variance\" or criterion_name == \"mad_median\":\n",
    "            raise NotImplemented\n",
    "        self.criterion_name = criterion_name\n",
    "\n",
    "        self.depth = 0\n",
    "        self.root = None # Use the Node class to initialize it later\n",
    "        self.debug = debug\n",
    "\n",
    "        \n",
    "        \n",
    "    def make_split(self, feature_index, threshold, X_subset, y_subset):\n",
    "        \"\"\"\n",
    "        Makes split of the provided data subset and target values using provided feature and threshold\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_index : int\n",
    "            Index of feature to make split with\n",
    "\n",
    "        threshold : float\n",
    "            Threshold value to perform split\n",
    "\n",
    "        X_subset : np.array of type float with shape (n_objects, n_features)\n",
    "            Feature matrix representing the selected subset\n",
    "\n",
    "        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n",
    "                   (n_objects, 1) in regression \n",
    "            One-hot representation of class labels for corresponding subset\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        (X_left, y_left) : tuple of np.arrays of same type as input X_subset and y_subset\n",
    "            Part of the providev subset where selected feature x^j < threshold\n",
    "        (X_right, y_right) : tuple of np.arrays of same type as input X_subset and y_subset\n",
    "            Part of the providev subset where selected feature x^j >= threshold\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        left_part = X_subset[:, feature_index] < threshold\n",
    "        right_part = X_subset[:, feature_index] >= threshold\n",
    "        \n",
    "        X_left, X_right = X_subset[left_part], X_subset[right_part]\n",
    "        y_left, y_right = y_subset[left_part], y_subset[right_part]\n",
    "        \n",
    "        return (X_left, y_left), (X_right, y_right)\n",
    "    \n",
    "    def make_split_only_y(self, feature_index, threshold, X_subset, y_subset):\n",
    "        \"\"\"\n",
    "        Split only target values into two subsets with specified feature and threshold\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_index : int\n",
    "            Index of feature to make split with\n",
    "\n",
    "        threshold : float\n",
    "            Threshold value to perform split\n",
    "\n",
    "        X_subset : np.array of type float with shape (n_objects, n_features)\n",
    "            Feature matrix representing the selected subset\n",
    "\n",
    "        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n",
    "                   (n_objects, 1) in regression \n",
    "            One-hot representation of class labels for corresponding subset\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_left : np.array of type float with shape (n_objects_left, n_classes) in classification \n",
    "                   (n_objects, 1) in regression \n",
    "            Part of the provided subset where selected feature x^j < threshold\n",
    "\n",
    "        y_right : np.array of type float with shape (n_objects_right, n_classes) in classification \n",
    "                   (n_objects, 1) in regression \n",
    "            Part of the provided subset where selected feature x^j >= threshold\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        left_part = X_subset[:, feature_index] < threshold\n",
    "        right_part = X_subset[:, feature_index] >= threshold\n",
    "        \n",
    "        y_left, y_right = y_subset[left_part], y_subset[right_part]\n",
    "        \n",
    "        return y_left, y_right\n",
    "\n",
    "    def choose_best_split(self, X_subset, y_subset):\n",
    "        \"\"\"\n",
    "        Greedily select the best feature and best threshold w.r.t. selected criterion\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_subset : np.array of type float with shape (n_objects, n_features)\n",
    "            Feature matrix representing the selected subset\n",
    "\n",
    "        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n",
    "                   (n_objects, 1) in regression \n",
    "            One-hot representation of class labels or target values for corresponding subset\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        feature_index : int\n",
    "            Index of feature to make split with\n",
    "\n",
    "        threshold : float\n",
    "            Threshold value to perform split\n",
    "\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        best_feature_index = 0\n",
    "        best_threshold = 0\n",
    "        best_criterion = float(\"inf\")\n",
    "        for feature_index in range(X_subset.shape[1]):\n",
    "            thresholds = np.unique(X_subset[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                y_left, y_right = self.make_split_only_y(feature_index, threshold, X_subset, y_subset)\n",
    "                if len(y_left) == 0 or len(y_right) == 0:\n",
    "                    continue\n",
    "                criterion = self.criterion(y_left) * len(y_left) + self.criterion(y_right) * len(y_right)\n",
    "                if criterion < best_criterion:\n",
    "                    best_criterion = criterion\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "                    \n",
    "        return best_feature_index, best_threshold\n",
    "    \n",
    "    def make_tree(self, X_subset, y_subset):\n",
    "        \"\"\"\n",
    "        Recursively builds the tree\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_subset : np.array of type float with shape (n_objects, n_features)\n",
    "            Feature matrix representing the selected subset\n",
    "\n",
    "        y_subset : np.array of type float with shape (n_objects, n_classes) in classification \n",
    "                   (n_objects, 1) in regression \n",
    "            One-hot representation of class labels or target values for corresponding subset\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        root_node : Node class instance\n",
    "            Node of the root of the fitted tree\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        self.depth += 1\n",
    "        if self.depth == self.max_depth or len(y_subset) < self.min_samples_split:\n",
    "            return None\n",
    "        feature_index, threshold = self.choose_best_split(X_subset, y_subset)\n",
    "        proba = np.sum(y_subset, axis=0) / np.sum(y_subset)\n",
    "        new_node = Node(feature_index, threshold, proba=proba)\n",
    "        left_child, right_child = self.make_split(feature_index, threshold, X_subset, y_subset)\n",
    "        new_node.left_child = self.make_tree(left_child[0], left_child[1])\n",
    "        new_node.right_child = self.make_tree(right_child[0], right_child[1])\n",
    "        return new_node\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model from scratch using the provided data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array of type float with shape (n_objects, n_features)\n",
    "            Feature matrix representing the data to train on\n",
    "\n",
    "        y : np.array of type int with shape (n_objects, 1) in classification \n",
    "                   of type float with shape (n_objects, 1) in regression \n",
    "            Column vector of class labels in classification or target values in regression\n",
    "        \n",
    "        \"\"\"\n",
    "        assert len(y.shape) == 2 and len(y) == len(X), 'Wrong y shape'\n",
    "        self.criterion, self.classification = self.all_criterions[self.criterion_name]\n",
    "        if self.classification:\n",
    "            if self.n_classes is None:\n",
    "                self.n_classes = len(np.unique(y))\n",
    "            y = one_hot_encode(self.n_classes, y)\n",
    "\n",
    "        self.root = self.make_tree(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target value or class label  the model from scratch using the provided data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array of type float with shape (n_objects, n_features)\n",
    "            Feature matrix representing the data the predictions should be provided for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_predicted : np.array of type int with shape (n_objects, 1) in classification \n",
    "                   (n_objects, 1) in regression \n",
    "            Column vector of class labels in classification or target values in regression\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        y_predicted = []\n",
    "        for x_object in X:\n",
    "            node = self.root\n",
    "            while (node.left_child is not None) and (node.right_child is not None):\n",
    "                if x_object[node.feature_index] < node.value:\n",
    "                    node = node.left_child\n",
    "                else:\n",
    "                    node = node.right_child\n",
    "            y_predicted.append(np.argmax(node.proba))\n",
    "        return np.array(y_predicted)\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Only for classification\n",
    "        Predict the class probabilities using the provided data\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.array of type float with shape (n_objects, n_features)\n",
    "            Feature matrix representing the data the predictions should be provided for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_predicted_probs : np.array of type float with shape (n_objects, n_classes)\n",
    "            Probabilities of each class for the provided objects\n",
    "        \n",
    "        \"\"\"\n",
    "        assert self.classification, 'Available only for classification problem'\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        y_predicted_probs = []\n",
    "        for x_object in X:\n",
    "            node = self.root\n",
    "            while (node.left_child is not None) and (node.right_child is not None):\n",
    "                if x_object[node.feature_index] < node.value:\n",
    "                    node = node.left_child\n",
    "                else:\n",
    "                    node = node.right_child\n",
    "            y_predicted_probs.append(node.proba)\n",
    "        return np.array(y_predicted_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 0: Initialization (0.01 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3dee5ee83e0f63188671f08b57d70804",
     "grade": true,
     "grade_id": "cell-3473b7b6ffd64d07",
     "locked": true,
     "points": 0.01,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do not change this cell\n",
    "import numpy as np\n",
    "import unittest\n",
    "import sys\n",
    "import io\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing, load_digits\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits_data = load_digits(n_class=2).data\n",
    "digits_target = load_digits(n_class=2).target[:, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Make splits loops (0.1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48b2963c650791df41dfbd190ed247fd",
     "grade": true,
     "grade_id": "cell-e3503c286039ec55",
     "locked": true,
     "points": 0.09,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X = np.ones((4, 5), dtype=float) * np.arange(4)[:, None]\n",
    "y = np.arange(4)[:, None] + np.asarray([0.2, -0.3, 0.1, 0.4])[:, None]\n",
    "class_estimator = DecisionTree(max_depth=5, criterion_name='gini')\n",
    "\n",
    "(X_l, y_l), (X_r, y_r) = class_estimator.make_split(1, 1., X, y)\n",
    "\n",
    "flag_X = np.array_equal(X[:1], X_l) and np.array_equal(X[1:], X_r) \n",
    "flag_y = np.array_equal(y[:1], y_l) and np.array_equal(y[1:], y_r)\n",
    "assert flag_X and flag_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Gini accuracy (0.2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a2a5e274d8d866e1242a339a7751642",
     "grade": true,
     "grade_id": "cell-e2c4124a6f815118",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6c2477a21f58>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclass_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gini'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclass_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0maccuracy_gini\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;36m0.96\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0maccuracy_gini\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "class_estimator = DecisionTree(max_depth=5, criterion_name='gini')\n",
    "class_estimator.fit(X_train, y_train)\n",
    "ans = class_estimator.predict(X_test)\n",
    "accuracy_gini = accuracy_score(y_test, ans)\n",
    "assert 0.96 < accuracy_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Entropy accuracy (0.2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3b167fd8a4950ffe1f1b8f691ab7f91",
     "grade": true,
     "grade_id": "cell-69473387a23d8dff",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1f250b0ec5b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclass_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclass_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;36m0.96\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "class_estimator = DecisionTree(max_depth=5, criterion_name='entropy')\n",
    "class_estimator.fit(X_train, y_train)\n",
    "ans = class_estimator.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, ans)\n",
    "assert 0.96 < accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Entropy probabilities (0.2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ea5402e3fc351fe4bdc17dc7d48b591",
     "grade": true,
     "grade_id": "cell-e5f59af66e6c111b",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2de65a748309>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mclass_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclass_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mreference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.48611111\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.51388889\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mreference\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "class_estimator = DecisionTree(max_depth=10, criterion_name='entropy')\n",
    "class_estimator.fit(X_train, y_train)\n",
    "ans = class_estimator.predict(X_test)\n",
    "reference = np.array([0.48611111, 0.51388889])\n",
    "assert np.abs(np.sum(class_estimator.predict_proba(X_test).mean(axis=0) - reference)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 5: MSE mad_median (0.15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "740411f734d1c9841d5fcc124eb129d1",
     "grade": true,
     "grade_id": "cell-1a9c1e3609ed9aab",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-c2da6202945a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mRX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRy_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRy_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregr_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mregressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mad_median'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRy_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mpredictions_mad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e9495c25cfb8>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_classes, max_depth, min_samples_split, criterion_name, debug)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcriterion_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"variance\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcriterion_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mad_median\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "random_indices = np.random.choice(np.arange(len(housing.data)), 500)\n",
    "\n",
    "regr_data = housing.data[random_indices]\n",
    "regr_target = housing.target[random_indices, None]\n",
    "RX_train, RX_test, Ry_train, Ry_test = train_test_split(regr_data, regr_target, test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTree(max_depth=8, criterion_name='mad_median')\n",
    "regressor.fit(RX_train, Ry_train)\n",
    "predictions_mad = regressor.predict(RX_test)\n",
    "mse = mean_squared_error(Ry_test, predictions_mad)\n",
    "assert 0.4 < mse < 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 6: MSE Variance (0.15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8ae6da572b6c3a76405ebfa4b9f4fd6",
     "grade": true,
     "grade_id": "cell-1ddb0377b6c68deb",
     "locked": true,
     "points": 0.15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8d879d450c2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mRX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRy_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRy_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregr_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mregressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'variance'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRy_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mpredictions_mad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-e9495c25cfb8>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_classes, max_depth, min_samples_split, criterion_name, debug)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcriterion_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"variance\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcriterion_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mad_median\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "housing = fetch_california_housing()\n",
    "random_indices = np.random.choice(np.arange(len(housing.data)), 500)\n",
    "\n",
    "regr_data = housing.data[random_indices]\n",
    "regr_target = housing.target[random_indices, None]\n",
    "RX_train, RX_test, Ry_train, Ry_test = train_test_split(regr_data, regr_target, test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTree(max_depth=8, criterion_name='variance')\n",
    "regressor.fit(RX_train, Ry_train)\n",
    "predictions_mad = regressor.predict(RX_test)\n",
    "mse = mean_squared_error(Ry_test, predictions_mad)\n",
    "assert 0.5 < mse < 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
